Preface of Debidas Ghosh, Introduction to Theory of Automata, Formal Languages, and Computation, 2013, PHI Learning Private Limited.

The theory of formal languages arose in the 1950s when computer scientists were trying to use computers to **translate one language into another**. Although not much was achieved at that time in the translation of natural languages, it paved the way for describing computer languages (such as ALGOL 60) later. In the mid-thirties, Turing machines were introduced to develop the Theory of Computability. It turned out that **Turing machines were the most general devices for accepting formal languages**.

Theory of computation has a range of application in compiler design, robotics, artificial intelligence (AI), and knowledge engineering.

The book begins with an overview of mathematical preliminaries and goes on to give a discussion on the introductory concepts of theory of automata, properties of regular sets and regular expressions, and the basics of formal languages.

**Finite automata** are introduced in Chapter 3. The reason for studying finite automata is their applicability to the design of several common types of computer algorithms and programs. For example, the lexical analsis phase of a compiler is often based on the simulation of a finite automaton. The problem of finding an occurrence of one string within another can also be solved efficiently by methods originating from the theory of finite automata.

The language accepted by finite automata is **regular language**. This language and its properties are covered in Chapters 4 and 5, respectively. **Regular grammars** and **regular expressions** and their relationship with regular languages are also discussed here. Besides, in Chapter 5 we deal with the identification of non-regular languages using the pumping lemma and also its closure properties with respect to various operations.

The most obvious limitation of finite automata is that it can keep track of its current state only. Thus is can recognize only simple languages. Context-free languages allow richer syntax than regular languages. They can be generated using context-free grammars and can be recognized by more powerful automata called **pushdown automata (a finite automaton with an auxiliary memory in the form of a stack)**.

**Context-free grammars** are important because of their ability to describe much of the syntax of high-level programming languages and other related formal languages. The corresponding machine, pushdown automata, provides a natural way to approach the problem of parsing a statement in high-level programming languages. It can determine the syntax of the statement by reconstructing the sequence of roles with the help of which it is derived from the context-free grammar. These ideas are covered in Chapters 6 and 7. Various properties of context-free languages are analyzed in Chapter 8.

The generalization of a pushdown automata is a **Turing machine**, which is described in Chapter 9. A Turing machine can be designed to accept quite complicated languages and to calculate the volumes of nontrivial arithmetic functions. A brief discussion of the Church-Turing Thesis is also presented here.

Chapter 10 deals with various models of Turing machines and gives a detailed explanation of their propertes and equivalence with the standard Turing machine. Chapter 11 covers **recursive and recursively enumerable languages** and shows that **unrestricted grammar generates recursively enumerable languages**. Further, the chapter deals with the **hierarchy of formal languages** and **linear bounded automata**. A question arises whether there are any languages not accepted by Turing machines. We answer this question first by showing **there are more languages than Turing machines**, so there must be some languages for which there are no Turing machines. In addition, we describe **context-sensitive languages generated by context-sensitive grammars and which are more powerful than context-free languages**. They can describe various problems which cannot be described by context-free languages. The corresponding machine, **linear bounded automata**, which accepts context-sensitive languages, are also described in detail. Finally, we discuss the **Chomsky hierarchy** defining the classification of various formal languages.

Chapter 12 covers **undecidability problems**. Here we show that many questions about Turing machines, as well as some questions about context-free languages and other formalists, have no algorithms for their solution. We also introduce some fundamental concepts from the theory of **recursive functions**, including the hierarchy of problems induced by consideration of a **Turing machine with oracles**.

Chapter 13 gives a clear exposition of the Theory of Computability, i.e. decidability and reducibility. **Decision problems** are those for which every specific instance can be answered in the form of a 'yes' or 'no' response. **Reducibility** deals with the technique of transforming an instance of a problem into an instance of another problem.

In Chapter 14, we present the basics of Computational Complexity Theory. First, we introduce a way of measuring the time used to solve a problem, i.e. the **time complexity** of an algorithm. Then, we show how to **classify problems** according to the amount of time required. We also discuss the possibility that certain decidable problems require enourmous amount of time, and how to tackle such problems. **Space complexity** shares many of the features of time complexity and serves as a further way of classifying problems according to their computational complexity. The real question is to select a model for measuring the space used by an algorithm.

I ardently hope this book would prove extremely useful as a text for students. I also trust that the academic fraternity will find the book valuable and interesting despite the availability of quite a fwe books on the subject. Finally, any constructive suggestions for improvement of the text would be most welcome.